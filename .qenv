# Inference Backend
# Force a specific inference backend instead of auto-detection
# Options: TENSORRT, CUDA, OPENVINO, DIRECTML, COREML, CPU, AUTO
# AUTO: Automatically detect best available backend (default)
# Leave commented or set to AUTO for automatic detection
# INFERENCE_BACKEND=AUTO

# Server Configuration
SERVER_PORT=1429
SERVER_HOST=0.0.0.0

# Detection Parameters
# Confidence threshold for bubble detection (0.0 - 1.0)
CONFIDENCE_THRESHOLD=0.3
# IoU threshold for non-maximum suppression (0.0 - 1.0)
IOU_THRESHOLD=0.7
# Target size for model input (pixels)
TARGET_SIZE=640

# Batch Processing
# Number of images to process in parallel
BATCH_SIZE=15

# Gemini API Configuration
# IMPORTANT: API keys should be in .env.local (not tracked in git)
# See .env.local.example for setup instructions
# Get your API keys from: https://aistudio.google.com/apikey
GEMINI_API_KEYS=AIzaSyBEEdHzMH6Ne6wMNxMVZVDV52sKDa32r3Q,AIzaSyCqGV0qnin5NgfFEqdPdeuo82_Ymip6CNI,AIzaSyDlYYsjFBKqNc28hL1Kg1_15AWHC8IgMYI,AIzaSyBCWvKiRetoz2OfukfThl2QVCGmC4vymKM,AIzaSyDPLAV4xOmK-d7vcQnF2Rru-oz35S88MpE,AIzaSyCFD1fzOOIl2YVhqTvuFMsqQomDI_seZI4

# Logging level: TRACE, DEBUG, INFO, WARN, ERROR
# Default: INFO (use DEBUG for troubleshooting)
LOG_LEVEL=INFO

# AI Models
TRANSLATION_MODEL=gemini-2.5-flash
# Maximum number of retries for API calls
MAX_RETRIES=3

# Rendering Configuration
# Upscale factor for text rendering
UPSCALE_FACTOR=3
# Padding ratio around text bubbles
PADDING_RATIO=0.05

# Cache Configuration
# Directory for storing translation cache
CACHE_DIR=.cache

BLUR_FREE_TEXT=true
