# ============================================================================
# MANGA TEXT PROCESSING WORKFLOW - CONFIGURATION
# ============================================================================
# This file documents all available configuration options.
#
# SETUP INSTRUCTIONS:
#   1. Copy this file to .env: cp .env.example .env
#   2. Copy .env.local.example to .env.local: cp .env.local.example .env.local
#   3. Add your API keys to .env.local (NOT .env - it may be tracked in git)
#   4. Adjust other settings in .env as needed
#
# ============================================================================

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# Options: TRACE, DEBUG, INFO, WARN, ERROR
# Default: INFO
# Use DEBUG for detailed API error messages, INFO for production
LOG_LEVEL=INFO

# ============================================================================
# INFERENCE BACKEND CONFIGURATION
# ============================================================================
# Force a specific inference backend instead of auto-detection
# Options: TENSORRT, CUDA, OPENVINO, DIRECTML, COREML, CPU, AUTO
# Default: AUTO (automatically detect best available backend)
#
# Platform-specific recommendations:
#   - Linux with NVIDIA GPU: CUDA or TENSORRT
#   - Linux with Intel GPU: OPENVINO
#   - Windows: DIRECTML or CUDA (if NVIDIA GPU)
#   - macOS Apple Silicon: COREML
#   - CPU-only: CPU
INFERENCE_BACKEND=AUTO

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================
# Port for the HTTP server
# Default: 1420
SERVER_PORT=1420

# Host address to bind to
# Default: 0.0.0.0 (all interfaces)
# Use 127.0.0.1 for localhost only
SERVER_HOST=0.0.0.0

# ============================================================================
# DETECTION PARAMETERS
# ============================================================================
# Confidence threshold for bubble detection (0.0 - 1.0)
# Higher values = fewer false positives, may miss some text
# Default: 0.3
CONFIDENCE_THRESHOLD=0.3

# IoU (Intersection over Union) threshold for non-maximum suppression (0.0 - 1.0)
# Higher values = allow more overlapping boxes
# Default: 0.7
IOU_THRESHOLD=0.7

# Target size for model input (pixels)
# Valid range: [320, 2048]
# Higher values = better accuracy but slower processing
# Default: 640
TARGET_SIZE=640

# Path to detector ONNX model
# Default: models/detector.onnx (embedded in binary)
# Override to use external model file
# DETECTOR_MODEL_PATH=models/detector.onnx

# Path to mask segmentation ONNX model
# Default: models/mask.onnx (embedded in binary)
# Override to use external model file
# MASK_MODEL_PATH=models/mask.onnx

# ============================================================================
# BATCH PROCESSING CONFIGURATION
# ============================================================================
# N: Number of images per batch (processed sequentially through phases)
# Batches themselves run in parallel (up to MAX_CONCURRENT_BATCHES)
# Default: 10
# Recommendation: 5-15 for balanced memory/throughput
BATCH_SIZE_N=10

# M: Number of images per API call for simple backgrounds
# When batching multiple images into a single Gemini API call for OCR/translation
# Default: 5
# Recommendation: 3-8 depending on API rate limits
API_BATCH_SIZE_M=5

# Maximum number of batches to process concurrently
# Higher values = more parallelism but more memory/GPU usage
# Default: 100
# Recommendation: Set to min(num_cpus * 10, 100) or tune based on memory
MAX_CONCURRENT_BATCHES=100

# NOTE: ONNX session pool size is automatically calculated
# Based on: min(num_cpus, MAX_CONCURRENT_BATCHES)
# No configuration needed - handled internally by the session pool

# ============================================================================
# API CONFIGURATION
# ============================================================================
# IMPORTANT: Put API keys in .env.local (not here!)
# See .env.local.example for setup instructions
# Comma-separated list of API keys for load balancing and redundancy
# Get your API keys from: https://aistudio.google.com/apikey
# GEMINI_API_KEYS=your_key_1,your_key_2,your_key_3

# Banana Mode: Use Gemini nano banana API for complex backgrounds
# When enabled: complex bg regions use image-to-image translation
# When disabled: all regions use traditional OCR + translation API
# Default: false
BANANA_MODE_ENABLED=false

# Model for traditional OCR/translation (simple backgrounds, and complex if banana disabled)
# Options: gemini-2.5-flash, gemini-flash-latest, gemini-2.5-pro, gemini-1.5-flash
# Default: gemini-2.5-flash
# Recommendation: gemini-2.5-flash for speed, gemini-2.5-pro for accuracy
OCR_TRANSLATION_MODEL=gemini-2.5-flash

# Model for banana mode (complex background image-to-image translation)
# Only used when BANANA_MODE_ENABLED=true
# Default: gemini-2.5-flash-image
BANANA_IMAGE_MODEL=gemini-2.5-flash-image

# Maximum number of retries for API calls
# Uses exponential backoff: 2^attempt seconds between retries
# Default: 3
# Recommendation: 3-5 for production
MAX_RETRIES=3

# HTTP timeout for API calls (seconds)
# Default: 60
# Recommendation: 30-120 depending on network conditions
API_TIMEOUT_SECONDS=60

# ============================================================================
# BACKGROUND CLASSIFICATION
# ============================================================================
# Threshold for simple background classification (0.0 - 1.0)
# If >= this percentage of label 0,1 region pixels are white, it's "simple background"
# Default: 0.6 (60%)
# Higher values = stricter definition of "simple" (more regions go to banana mode)
SIMPLE_BG_WHITE_THRESHOLD=0.6

# ============================================================================
# RENDERING CONFIGURATION
# ============================================================================
# Upscale factor for text rendering (higher = better quality, slower)
# Valid range: [1, 10]
# Default: 3
# Recommendation: 2-4 for balanced quality/speed
UPSCALE_FACTOR=3

# Text stroke/outline for better readability
# Enable white outline around text (comic book style)
# Default: false
TEXT_STROKE_ENABLED=false

# Stroke width in pixels (only used when TEXT_STROKE_ENABLED=true)
# Default: 2
# Recommendation: 1-3 depending on font size
TEXT_STROKE_WIDTH=2

# Free text (label 2) background handling
# When true: blur background behind free text
# When false: use solid white background
# Default: false
BLUR_FREE_TEXT=false

# Blur radius in pixels (only used when BLUR_FREE_TEXT=true)
# Default: 10
# Recommendation: 5-20 depending on desired effect
BLUR_RADIUS=10

# ============================================================================
# CACHE CONFIGURATION
# ============================================================================
# Directory for storing translation cache
# Cache persists translations to avoid redundant API calls
# Default: .cache
CACHE_DIR=.cache

# Cache persistence interval (seconds)
# How often to save cache to disk (0 = save immediately on every change)
# Default: 30
# Recommendation: 30-300 for balanced performance/safety
# CACHE_SAVE_INTERVAL_SECONDS=30

# Maximum cache size (number of entries)
# Oldest entries are evicted when limit is reached (LRU)
# Default: 10000
# CACHE_MAX_ENTRIES=10000

# ============================================================================
# RATE LIMITING
# ============================================================================
# Enable rate limiting for HTTP endpoints
# Protects server from being overwhelmed
# Default: false (disabled)
RATE_LIMIT_ENABLED=false

# Maximum requests allowed per window
# Default: 100
RATE_LIMIT_MAX_REQUESTS=100

# Time window in seconds for rate limiting
# Default: 60 (1 minute)
RATE_LIMIT_WINDOW_SECONDS=60

# ============================================================================
# CIRCUIT BREAKER (API Resilience)
# ============================================================================
# Enable circuit breaker pattern for API calls
# Prevents cascading failures by failing fast when API is down
# Default: true
# CIRCUIT_BREAKER_ENABLED=true

# Number of consecutive failures before opening circuit
# Default: 5
# CIRCUIT_BREAKER_FAILURE_THRESHOLD=5

# How long to wait before attempting recovery (seconds)
# Default: 60
# CIRCUIT_BREAKER_TIMEOUT_SECONDS=60

# Number of successful calls needed to close circuit again
# Default: 3
# CIRCUIT_BREAKER_SUCCESS_THRESHOLD=3

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
# Enable batch pipelining (experimental)
# Allows next batch to start Phase 1 while current batch is in Phase 2+
# Default: false
# BATCH_PIPELINING_ENABLED=false

# Number of images to process with spawn_blocking for image operations
# Threshold below which image ops run synchronously
# Default: 5
# IMAGE_OPS_BLOCKING_THRESHOLD=5

# ============================================================================
# MONITORING & METRICS
# ============================================================================
# Enable Prometheus metrics endpoint at /metrics
# Default: false
# METRICS_ENABLED=false

# Enable detailed performance tracking
# Adds overhead but provides insights into bottlenecks
# Default: false
# DETAILED_METRICS_ENABLED=false

# ============================================================================
# LEGACY CONFIGURATION (deprecated, use specific settings above)
# ============================================================================
# BATCH_SIZE is deprecated, use BATCH_SIZE_N instead
# TRANSLATION_MODEL is deprecated, use OCR_TRANSLATION_MODEL instead

# ============================================================================
# NOTE: Cache persistence is always enabled
# ============================================================================
# Cache is automatically persisted to disk in the CACHE_DIR
# Format: .cache.json.gz (compressed for efficiency)
# Cache survives server restarts and provides significant performance gains
